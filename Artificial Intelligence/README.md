# äººå·¥æ™ºèƒ½é¢†åŸŸè®ºæ–‡æ¨è

æ•´ç†äº†äººå·¥æ™ºèƒ½æ ¸å¿ƒé¢†åŸŸçš„ç»å…¸ä¸å‰æ²¿è®ºæ–‡ï¼Œè¦†ç›–å¤§æ¨¡å‹ã€åŸºç¡€æ¶æ„ã€å¤šæ¨¡æ€ã€åº”ç”¨è½åœ°ç­‰æ–¹å‘ï¼Œé€‚åˆå…¥é—¨å­¦ä¹ ä¸æ·±åº¦ç ”ç©¶å‚è€ƒï¼š

## **ä¸€ã€å¤§æ¨¡å‹ä¸åŸºç¡€æ¶æ„**

### 1. Transformä¸‡ç‰©èµ·æº

- **è®ºæ–‡**ï¼š[ğŸ”¥*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)
- **ç®€ä»‹**ï¼šTransformer æ¶æ„å¥ åŸºä½œï¼Œç”¨æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£ä¼ ç»Ÿå¾ªç¯/å·ç§¯ç»“æ„ï¼Œå¼€å¯å¤§æ¨¡å‹æ—¶ä»£ï¼ˆBERTã€GPT å‡åŸºäºæ­¤æ‹“å±•ï¼‰ã€‚

### 2. Bert é¢„è®­ç»ƒé‡Œç¨‹ç¢‘

- **è®ºæ–‡**ï¼š[ğŸ”¥*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805)
- **ç®€ä»‹**ï¼šåŒå‘é¢„è®­ç»ƒ + æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼Œå¼€å¯ NLP é¢„è®­ç»ƒèŒƒå¼

### 3. GPT ç”Ÿæˆå¼å¤§æ¨¡å‹ä»£è¡¨
- **è®ºæ–‡**ï¼š[ğŸ”¥*GPT-3: Language Models are Few-Shot Learners*](https://arxiv.org/abs/2005.14165)
- **è®ºæ–‡**ï¼š[ğŸ”¥*GPT-1: Improving Language Understanding by Generative Pre-Training*](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- **ç®€ä»‹**ï¼šå•å‘é¢„è®­ç»ƒ + è‡ªå›å½’ç”Ÿæˆï¼ŒéªŒè¯ â€œå¤§å‚æ•° + å°‘æ ·æœ¬â€ èƒ½åŠ›

### 4. T5 ç»Ÿä¸€æ¡†æ¶
- **è®ºæ–‡**ï¼š[ğŸ”¥*T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*](https://arxiv.org/abs/1910.10683)
- **ç®€ä»‹**ï¼šæå‡º â€œText-to-Textâ€ ç»Ÿä¸€æ¡†æ¶ï¼Œå°†æ‰€æœ‰ NLP ä»»åŠ¡è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆ

### 2. Deepseekç³»åˆ—

- **è®ºæ–‡**ï¼š[ğŸ”¥*DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*](https://arxiv.org/abs/2501.12948)
- **ç®€ä»‹**ï¼šåŸºäº Transformer æ¶æ„ï¼Œå¼€å‘äº†åŸºäºå¤§æ¨¡å‹æ£€ç´¢çš„ LLM ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨å¤šæ¨¡æ€ã€è·¨é¢†åŸŸã€å¤šä»»åŠ¡ã€å¤šæ¨¡æ€å¤šä»»åŠ¡ç­‰åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€ã€‚

## **äºŒã€è®­ç»ƒä¼˜åŒ–ä¸æ•ˆç‡æå‡**

### 1. å¤§è§„æ¨¡è®­ç»ƒæŠ€æœ¯
- **è®ºæ–‡**:[ğŸ”¥Efficient Large-Scale Language Model Training on GPU Clustersï¼ˆMegatron-LM](https://arxiv.org/abs/2104.04473)
- **ç®€ä»‹**:Megatron-LM æ¡†æ¶ï¼ŒåŸºäºå•æœºå¤šå¡ã€å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒï¼Œå®ç°å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚è´¡çŒ®ï¼šæ··åˆç²¾åº¦è®­ç»ƒã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œç­‰å·¥ç¨‹ä¼˜åŒ–ã€‚
- 
### 2. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰
- **è®ºæ–‡**:[ğŸ”¥LoRAï¼šLoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **è®ºæ–‡**:[ğŸ”¥QLoRAï¼šQLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- **ç®€ä»‹**:ç”¨ä½ç§©é€‚åº”ã€é‡åŒ–æŠ€æœ¯é™ä½å¾®è°ƒæˆæœ¬ï¼ˆå¦‚åœ¨æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒï¼‰ã€‚
- 
### 3. é¢„è®­ç»ƒæ•°æ®ä¸æ‰©å±•å®šå¾‹
- **è®ºæ–‡**:[ğŸ”¥Scaling Lawsï¼šScaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- **ç®€ä»‹**:æ­ç¤ºæ¨¡å‹æ€§èƒ½ä¸å‚æ•°é‡ã€æ•°æ®é‡çš„å…³ç³»ï¼ŒæŒ‡å¯¼æ¨¡å‹è®¾è®¡ã€‚

## **ä¸‰ã€å¤šæ¨¡æ€ä¸è·¨é¢†åŸŸèåˆ**
### 1. è§†è§‰ä¸è¯­è¨€èåˆ

- **è®ºæ–‡**:[ğŸ”¥*ViTï¼šAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929)
- **ç®€ä»‹**:å°† Transformer ç›´æ¥ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œè¯æ˜çº¯æ³¨æ„åŠ›æœºåˆ¶å¯åª²ç¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæ¨åŠ¨å¤šæ¨¡æ€å¤§æ¨¡å‹å‘å±•ã€‚
- **è®ºæ–‡**:[ğŸ”¥CLIPï¼šLearning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- **ç®€ä»‹**:å°† Transformer å¼•å…¥ CV é¢†åŸŸï¼Œæ‰“é€šæ–‡æœ¬ä¸å›¾åƒç†è§£ã€‚

### 2. å¤šæ¨¡æ€å¤§æ¨¡å‹
- **è®ºæ–‡**:[ğŸ”¥GPT-4Vï¼šGPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
- **è®ºæ–‡**:[ğŸ”¥Qwen2-VLï¼šQwen2-VL: Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution](https://arxiv.org/abs/2305.11401)
- **ç®€ä»‹**ï¼šæ”¯æŒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥ä¸ç”Ÿæˆã€‚


